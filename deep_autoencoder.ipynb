{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import scipy.sparse as sp\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv('movies.csv')\n",
    "ratings_full = pd.read_csv('ratings.csv')\n",
    "# display(movies)\n",
    "# display(ratings)\n",
    "ratings = ratings_full[(ratings_full['userId'] <= 10000)]\n",
    "train_data, test_data = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147880044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147868828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>665</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1147878820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>899</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1147868510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496607</th>\n",
       "      <td>10000</td>\n",
       "      <td>117176</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1539043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496608</th>\n",
       "      <td>10000</td>\n",
       "      <td>118696</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1539035406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496609</th>\n",
       "      <td>10000</td>\n",
       "      <td>145150</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1539036026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496610</th>\n",
       "      <td>10000</td>\n",
       "      <td>157296</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1539034792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496611</th>\n",
       "      <td>10000</td>\n",
       "      <td>179135</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1539034355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1496612 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         userId  movieId  rating   timestamp\n",
       "0             1      296     5.0  1147880044\n",
       "1             1      306     3.5  1147868817\n",
       "2             1      307     5.0  1147868828\n",
       "3             1      665     5.0  1147878820\n",
       "4             1      899     3.5  1147868510\n",
       "...         ...      ...     ...         ...\n",
       "1496607   10000   117176     5.0  1539043478\n",
       "1496608   10000   118696     2.5  1539035406\n",
       "1496609   10000   145150     5.0  1539036026\n",
       "1496610   10000   157296     2.0  1539034792\n",
       "1496611   10000   179135     4.5  1539034355\n",
       "\n",
       "[1496612 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users: 10000\n",
      "num_movies 209171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_18104271/ipykernel_30898/4247757787.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ratings['userId'] = pd.to_numeric(ratings['userId'], errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "## Get number of users and movies\n",
    "display(ratings)\n",
    "ratings['userId'] = pd.to_numeric(ratings['userId'], errors='coerce')\n",
    "num_users = ratings['userId'].max()\n",
    "num_movies = movies['movieId'].max()\n",
    "total_movies = movies['movieId'].shape[0]\n",
    "print(\"num_users:\", num_users)\n",
    "print(\"num_movies\", num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try this autoencoder setup first \n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_movies, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, 512)\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.fc6 = nn.Linear(512, num_movies)\n",
    "        self.activation = nn.SELU()\n",
    "        self.dropout = nn.Dropout(0.8)\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc4(x))\n",
    "        x = self.activation(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(sae.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/SLURM_18104271/ipykernel_30898/750539457.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = torch.tensor(batch, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.18264053018676424\n",
      "epoch: 2 loss: 0.1822651043455633\n",
      "epoch: 3 loss: 0.18168078380021177\n",
      "epoch: 4 loss: 0.1809010857604327\n",
      "epoch: 5 loss: 0.17958289038883324\n",
      "epoch: 6 loss: 0.17721707305918702\n",
      "epoch: 7 loss: 0.17293273720090768\n",
      "epoch: 8 loss: 0.1653858336248635\n",
      "epoch: 9 loss: 0.15347730783012212\n",
      "epoch: 10 loss: 0.1400416936687967\n",
      "epoch: 11 loss: 0.13686165614553858\n",
      "epoch: 12 loss: 0.1325020398697864\n",
      "epoch: 13 loss: 0.11121897541559107\n",
      "epoch: 14 loss: 0.10917686787532886\n",
      "epoch: 15 loss: 0.1114080721665828\n",
      "epoch: 16 loss: 0.10516337917747083\n",
      "epoch: 17 loss: 0.09710179696455688\n",
      "epoch: 18 loss: 0.09601502797578128\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 18\n",
    "# batch_size = 50\n",
    "# train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "unique_user_ids = train_data['userId'].unique()\n",
    "# print(unique_user_ids)\n",
    "# Specify the batch size\n",
    "batch_size = 128 # Adjust this based on your preference\n",
    "# display(train_data)\n",
    "# Initialize the DataLoader with the unique user IDs\n",
    "user_loader = DataLoader(dataset=unique_user_ids, batch_size=batch_size, shuffle=True)\n",
    "# create the dataset of 32 and then just dataloader the whole set?\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(1, len(unique_user_ids), batch_size): \n",
    "        # Extract ratings for all users in mini-batch from ratings df \n",
    "        batch_user_ratings = train_data[(train_data['userId']>= id_user) & (train_data['userId'] < id_user + batch_size)] \n",
    "        # Transform df into matrix with users as rows and movieids as columns, with ratings as elements \n",
    "        batch_ratings_array = []\n",
    "        batch_unique_user_ids = batch_user_ratings['userId'].unique()\n",
    "        # print(batch_unique_user_ids)\n",
    "        for userid in batch_unique_user_ids:  \n",
    "            ratings_array = np.zeros(num_movies) # initialize zero array with number of movies length\n",
    "            user_ratings = batch_user_ratings[(batch_user_ratings['userId'] == userid)] # get this user's df section only\n",
    "            movie_ids = user_ratings['movieId'].values # get all the movies rated by this user \n",
    "            user_ratings_array = user_ratings['rating'].values # get all the ratings for those movies \n",
    "            ratings_array[movie_ids - 1] = user_ratings_array # place ratings in corresponding movie's index \n",
    "            # print(\"ratings_array: \", ratings_array)\n",
    "\n",
    "            # non_zero_mean = np.mean(ratings_array[ratings_array != 0])\n",
    "            # # Subtract mean from non-zero elements\n",
    "            # ratings_array[ratings_array != 0] -= non_zero_mean\n",
    "            # print(\"normalized: \", ratings_array)\n",
    "            batch_ratings_array.append(ratings_array)\n",
    "        # print(batch_ratings_array)\n",
    "        user_loader = DataLoader(dataset=batch_ratings_array, batch_size=batch_size, shuffle=True)\n",
    "        # user_ratings = ratings.loc[ratings['userId'] == id_user] # df for this specific user \n",
    "        # Create an array with zeros for all movies\n",
    "        i = 0\n",
    "        for batch in user_loader: \n",
    "        # input = torch.tensor(ratings_array, dtype=torch.float)\n",
    "        # target = input.clone()\n",
    "            # print(i)\n",
    "            i+=1\n",
    "            input = torch.tensor(batch, dtype=torch.float)\n",
    "            target = input.clone()\n",
    "            if torch.sum(target.data > 0) > 0:\n",
    "                output = sae(input)\n",
    "                target.require_grad = False\n",
    "                output[target == 0] = 0 # mask\n",
    "                loss = criterion(output, target)\n",
    "                # print(loss)\n",
    "                #  adjust the loss based on the density of the target values. \n",
    "                # If there are more zero values (sparse data), scale the loss to account for this sparsity.\n",
    "                mean_corrector = total_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "                loss.backward()\n",
    "                train_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "                # print(\"train_loss: \", train_loss)\n",
    "                s += 1.\n",
    "                # print(\"s: \", s)\n",
    "                # sae.fc4.weight.data = sae.fc1.weight.data.t().clone()\n",
    "                # sae.fc3.weight.data = sae.fc2.weight.data.t().clone()\n",
    "\n",
    "                optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'trained_model.sav'\n",
    "pickle.dump(sae,open(filename, 'wb'))\n",
    "loaded_sae = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rms_avg:  1.7112779033636827\n",
      "rms_avg:  1.9375000917571854\n",
      "rms_avg:  1.9494997627058108\n",
      "rms_avg:  1.8918221925898926\n",
      "rms_avg:  1.9480376375327375\n",
      "rms_avg:  1.814888585646851\n",
      "rms_avg:  1.9340226011454542\n",
      "rms_avg:  1.8730611668334298\n",
      "rms_avg:  1.8311533777787121\n",
      "rms_avg:  1.7911163127693954\n",
      "rms_avg:  1.8825558623030885\n",
      "rms_avg:  1.9903816031045514\n",
      "rms_avg:  1.8944121067091002\n",
      "rms_avg:  1.7190404137391422\n",
      "rms_avg:  1.9416817506153397\n",
      "rms_avg:  1.8195304710849192\n",
      "rms_avg:  1.7765829071851926\n",
      "rms_avg:  1.6512702771993388\n",
      "rms_avg:  1.7300126067935122\n",
      "rms_avg:  1.8854929254388892\n",
      "rms_avg:  1.9037463854661127\n",
      "rms_avg:  1.7573743800068466\n",
      "rms_avg:  1.7518519986732217\n",
      "rms_avg:  1.8231412462294574\n",
      "rms_avg:  1.687636913540427\n",
      "rms_avg:  1.5731737438553384\n",
      "rms_avg:  1.7511528555825635\n",
      "rms_avg:  1.8021028061665836\n",
      "rms_avg:  1.5586577505042698\n",
      "rms_avg:  1.692544317296755\n",
      "rms_avg:  1.6767507612044668\n",
      "rms_avg:  1.6574508355639166\n",
      "rms_avg:  1.8224929290713874\n",
      "rms_avg:  1.8380648645357305\n",
      "rms_avg:  1.6811405596003366\n",
      "rms_avg:  1.7358553127606982\n",
      "rms_avg:  1.730477151372184\n",
      "rms_avg:  1.6808063150810548\n",
      "rms_avg:  1.6535545625538643\n",
      "rms_avg:  1.7130135886089055\n",
      "rms_avg:  1.8229059197544855\n",
      "rms_avg:  1.7689097804276326\n",
      "rms_avg:  1.8079561363056234\n",
      "rms_avg:  1.4434747550528857\n",
      "rms_avg:  1.7573629556147088\n",
      "rms_avg:  1.8280160033472597\n",
      "rms_avg:  1.5806288217571585\n",
      "rms_avg:  1.5612810074639385\n",
      "rms_avg:  1.5951913000272495\n",
      "rms_avg:  1.661810989465232\n",
      "rms_avg:  1.736960587815805\n",
      "rms_avg:  1.7943644487364039\n",
      "rms_avg:  1.6570372744528228\n",
      "rms_avg:  1.7957242088961884\n",
      "rms_avg:  1.6973602704922854\n",
      "rms_avg:  1.6898666164238403\n",
      "rms_avg:  1.7662824645956234\n",
      "rms_avg:  1.6724074632410728\n",
      "rms_avg:  1.6030543010192286\n",
      "rms_avg:  1.6445695709458696\n",
      "rms_avg:  1.613288423497632\n",
      "rms_avg:  1.6432715380215377\n",
      "rms_avg:  1.5512588028865857\n",
      "rms_avg:  1.549708596182428\n",
      "rms_avg:  1.7545843831088601\n",
      "rms_avg:  1.607874688271703\n",
      "rms_avg:  1.7636571152424434\n",
      "rms_avg:  1.7865547877737595\n",
      "rms_avg:  1.5546210103787172\n",
      "rms_avg:  1.4892611555887079\n",
      "rms_avg:  1.5750461147581907\n",
      "rms_avg:  1.475121311630702\n",
      "rms_avg:  1.6769862433870852\n",
      "rms_avg:  1.5857960690011723\n",
      "rms_avg:  1.5647382825720049\n",
      "rms_avg:  1.8210587503229017\n",
      "rms_avg:  1.5019683060633964\n",
      "rms_avg:  1.7265891360323542\n",
      "rms_avg:  1.6502849531681636\n",
      "rms_avg:  1.524212615106717\n",
      "rms_avg:  1.6183859547584032\n",
      "rms_avg:  1.643838884213323\n",
      "rms_avg:  1.6208990771593292\n",
      "rms_avg:  1.8156959483817723\n",
      "rms_avg:  1.4964593406601159\n",
      "rms_avg:  1.6752249059586932\n",
      "rms_avg:  1.86655200044577\n",
      "rms_avg:  1.5705218940685521\n",
      "rms_avg:  1.5216664462583482\n",
      "rms_avg:  1.5649481598588013\n",
      "rms_avg:  1.8382551462435779\n",
      "rms_avg:  1.6649036935008292\n",
      "rms_avg:  1.749598256908776\n",
      "rms_avg:  1.5569914361454176\n",
      "rms_avg:  1.6041639802580385\n",
      "rms_avg:  1.6126899735305358\n",
      "rms_avg:  1.4574996388707415\n",
      "rms_avg:  1.5831697596308933\n",
      "rms_avg:  1.5978081290252533\n",
      "rms_avg:  1.6257668864463348\n",
      "rms_avg:  1.5851538461548293\n",
      "rms_avg:  1.654258984037737\n",
      "rms_avg:  1.5399388855340914\n",
      "rms_avg:  1.5337064335254214\n",
      "rms_avg:  1.729335549854585\n",
      "rms_avg:  1.6069385670306988\n",
      "rms_avg:  1.7725724484587122\n",
      "rms_avg:  1.6669755800559742\n",
      "rms_avg:  1.480965772404067\n",
      "rms_avg:  1.2791269591666408\n",
      "rms_avg:  1.4750898241654706\n",
      "rms_avg:  1.4769510862536106\n",
      "rms_avg:  1.6066686745635268\n",
      "rms_avg:  1.7255658947390797\n",
      "rms_avg:  1.5165212052642034\n",
      "rms_avg:  1.3410262910246749\n",
      "rms_avg:  1.5733177926507207\n",
      "rms_avg:  1.6330233695781766\n",
      "rms_avg:  1.5285526059475012\n",
      "rms_avg:  1.5107598954046437\n",
      "rms_avg:  1.6335357547335037\n",
      "rms_avg:  1.3766444000849745\n",
      "rms_avg:  1.3895378214706064\n",
      "rms_avg:  1.6845611797922286\n",
      "rms_avg:  1.578701556561422\n",
      "rms_avg:  1.5010782206622013\n",
      "rms_avg:  1.5910010051282677\n",
      "rms_avg:  1.610155494999794\n",
      "rms_avg:  1.5164751509042997\n",
      "rms_avg:  1.5134768353311898\n",
      "rms_avg:  1.6458232544705538\n",
      "rms_avg:  1.324468819309982\n",
      "rms_avg:  1.520294280989568\n",
      "rms_avg:  1.3633353939026351\n",
      "rms_avg:  1.515210701441118\n",
      "rms_avg:  1.4297189213723303\n",
      "rms_avg:  1.34705149335119\n",
      "rms_avg:  1.4667354592197654\n",
      "rms_avg:  1.4182958410753845\n",
      "rms_avg:  1.462919202095123\n",
      "rms_avg:  1.539610130897329\n",
      "rms_avg:  1.476046497097883\n",
      "rms_avg:  1.548052729045006\n",
      "rms_avg:  1.392284580864211\n",
      "rms_avg:  1.4164170762198962\n",
      "rms_avg:  1.5531320159063793\n",
      "rms_avg:  1.5544709201172238\n",
      "rms_avg:  1.5912874162203439\n",
      "rms_avg:  1.539242588144122\n",
      "rms_avg:  1.4422201189873725\n",
      "rms_avg:  1.6106130178459885\n",
      "rms_avg:  1.3604897307807884\n",
      "rms_avg:  1.3064529645289655\n",
      "rms_avg:  1.470600131181008\n",
      "rms_avg:  1.4798436663965202\n",
      "rms_avg:  1.5303440348277308\n",
      "rms_avg:  1.3723708791879305\n",
      "rms_avg:  1.4157638976738263\n",
      "rms_avg:  1.3569750798989133\n",
      "rms_avg:  1.5545721529173882\n",
      "rms_avg:  1.4103779681483157\n",
      "rms_avg:  1.4137668349424843\n",
      "rms_avg:  1.3531833128867046\n",
      "rms_avg:  1.4309886613603184\n",
      "rms_avg:  1.3625331111938488\n",
      "rms_avg:  1.3344403095593815\n",
      "rms_avg:  1.5551065188316264\n",
      "rms_avg:  1.4687935758476596\n",
      "rms_avg:  1.2717656261387909\n",
      "rms_avg:  1.2974800012587804\n",
      "rms_avg:  1.1662975142519219\n",
      "rms_avg:  1.3908890527116236\n",
      "rms_avg:  1.4666957426391718\n",
      "rms_avg:  1.240213344453074\n",
      "rms_avg:  1.25450204747076\n",
      "rms_avg:  1.3695179759734708\n",
      "rms_avg:  1.3984720432376962\n",
      "rms_avg:  1.3671573052664396\n",
      "rms_avg:  1.3653729797914906\n",
      "rms_avg:  1.395011611108252\n",
      "rms_avg:  1.4269378575960747\n",
      "rms_avg:  1.3124178852736503\n",
      "rms_avg:  1.3624598804334667\n",
      "rms_avg:  1.2335616599973749\n",
      "rms_avg:  1.0040004430579321\n",
      "rms_avg:  1.1257505010270052\n",
      "rms_avg:  1.1881534595190606\n",
      "rms_avg:  1.3867090340087698\n",
      "rms_avg:  1.2665950102141958\n",
      "rms_avg:  1.4724475405983193\n",
      "rms_avg:  1.2082384261696508\n",
      "rms_avg:  1.3979670395508046\n",
      "rms_avg:  1.500395533297227\n",
      "rms_avg:  1.3599466626539585\n",
      "rms_avg:  1.2589886848187475\n",
      "rms_avg:  1.402307166698131\n",
      "rms_avg:  1.4345044234598225\n",
      "rms_avg:  1.4416031691850648\n",
      "rms_avg:  1.0625153013638087\n",
      "i count:  33\n",
      "final avg rms:  1.8680553436279297\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "s = 0.\n",
    "# need to use train_data in above too \n",
    "# display(test_data)\n",
    "unique_test_user_ids = test_data['userId'].unique()\n",
    "rms_avg = []\n",
    "counter = 0\n",
    "sae.eval()\n",
    "for id_user in unique_test_user_ids:\n",
    "    test_ratings_array = np.zeros(num_movies) # initialize zero array with number of movies length\n",
    "    test_user_ratings = test_data[(test_data['userId'] == id_user)] # get this user's df section only\n",
    "    test_movie_ids = test_user_ratings['movieId'].values # get all the movies rated by this user \n",
    "    test_user_ratings_array = test_user_ratings['rating'].values # get all the ratings for those movies \n",
    "    test_ratings_array[test_movie_ids - 1] = test_user_ratings_array # place rating\n",
    "    nonzero_indices = np.nonzero(test_ratings_array)\n",
    "    # print(\"i count is: \", counter)\n",
    "    counter+=1\n",
    "\n",
    "    # print(\"nonzero indices: \", nonzero_indices)\n",
    "    zeroed_ratings = []\n",
    "    zeroed_ratings_indices = []\n",
    "    for i in range(0, len(nonzero_indices[0]), 8): \n",
    "        zeroed_ratings.append(test_ratings_array[nonzero_indices[0][i]])\n",
    "        zeroed_ratings_indices.append(nonzero_indices[0][i])\n",
    "        test_ratings_array[nonzero_indices[0][i]] = 0\n",
    "    # print(\"zeroed ratings indices\", zeroed_ratings_indices)\n",
    "\n",
    "    # print(\"zeroed ratings\", zeroed_ratings)\n",
    "    input = torch.tensor(test_ratings_array, dtype=torch.float)\n",
    "    # print(input)\n",
    "    target = input.clone()\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        input.require_grad = False \n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output_np = output.detach().numpy()\n",
    "        predicted_ratings = output_np[zeroed_ratings_indices]\n",
    "        predicted_capped_ratings = torch.clamp(torch.from_numpy(predicted_ratings), min=1, max=5)\n",
    "        predicted_capped_ratings_np = predicted_capped_ratings.numpy()\n",
    "        # print(\"predicted ratings: \", predicted_capped_ratings)\n",
    "        rms = np.sqrt(np.sum((predicted_ratings - zeroed_ratings)**2)/len(predicted_ratings))\n",
    "        # print(\"rms: \", rms)\n",
    "        rms_avg.append(rms)\n",
    "        # output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = total_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
    "        s += 1.\n",
    "    if counter == 50: \n",
    "        # print(\"in here\")\n",
    "        print(\"rms_avg: \", np.mean(rms_avg))\n",
    "        counter = 0\n",
    "        rms_avg = []\n",
    "# print('test loss: '+str(test_loss/s))\n",
    "print(\"i count: \", counter)\n",
    "print(\"final avg rms: \", np.mean(rms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
